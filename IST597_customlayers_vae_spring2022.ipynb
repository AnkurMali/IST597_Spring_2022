{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST597_customlayers_vae_spring2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_customlayers_vae_spring2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zYrSxp9d5p"
      },
      "source": [
        "# IST597 :- Custom layers and VAEs\n",
        "In this assignment we will learn, how to define your own custom layers using keras and integrate it with keras API. We will be developing simple Variational autoencoder and test it on MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLFys0y99dAt"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCeBeCyWAGR-"
      },
      "source": [
        "The full list of pre-existing layers can be seen in the documentation ([Keras API](https://https://www.tensorflow.org/api_docs/python/tf/keras/layers)). It includes Dense (a fully-connected layer), Conv2D (1D, 3D), RNN (GRU, LSTM ,etc), BatchNormalization, Dropout, and many others.)).\n",
        "\n",
        "Let's look at different way of defining layers using keras and how we can create custom layers if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjHPoCDBA8BP"
      },
      "source": [
        "layer = tf.keras.layers.Dense(256) # Provide number of hidden units, input shape is inferred from data\n",
        "layer = tf.keras.layers.Dense(128, input_shape=(None, 10)) # Provide input shape, if model is complex"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mdY3j8BVQn",
        "outputId": "d63d869c-b551-41b9-802a-4bc0a56e5071"
      },
      "source": [
        "layer(tf.zeros([120, 10]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(120, 128), dtype=float32, numpy=\n",
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZbTFC6NBaQI",
        "outputId": "383ea713-b570-43fc-9f3c-474270f76814"
      },
      "source": [
        "layer(tf.zeros([300, 10]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(300, 128), dtype=float32, numpy=\n",
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P_0kQ6EBpfJ",
        "outputId": "07d84a35-3cab-4474-ed7a-286f8e716c22"
      },
      "source": [
        "layer.variables # List all trainable variables"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_1/kernel:0' shape=(10, 128) dtype=float32, numpy=\n",
              " array([[-0.00174594,  0.07600002,  0.20278783, ..., -0.04090717,\n",
              "          0.0626701 ,  0.16592263],\n",
              "        [ 0.02887262,  0.10382028, -0.16075982, ...,  0.15742756,\n",
              "          0.06267197,  0.17785825],\n",
              "        [ 0.0235547 , -0.02224658, -0.19540118, ..., -0.16289224,\n",
              "          0.10448419,  0.11331768],\n",
              "        ...,\n",
              "        [-0.10184544, -0.06701   ,  0.12802236, ...,  0.09268863,\n",
              "         -0.16879267, -0.09703728],\n",
              "        [-0.19247532, -0.13628536,  0.10741906, ...,  0.18835728,\n",
              "          0.03636011, -0.0418143 ],\n",
              "        [ 0.07045038, -0.07457916, -0.20245764, ..., -0.14473625,\n",
              "          0.0724908 , -0.08168623]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApyIjeaWByTO",
        "outputId": "10926a6f-dac3-494b-9df7-005f08486527"
      },
      "source": [
        "layer.kernel, layer.bias #Check weight and biases of your model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Variable 'dense_1/kernel:0' shape=(10, 128) dtype=float32, numpy=\n",
              " array([[-0.00174594,  0.07600002,  0.20278783, ..., -0.04090717,\n",
              "          0.0626701 ,  0.16592263],\n",
              "        [ 0.02887262,  0.10382028, -0.16075982, ...,  0.15742756,\n",
              "          0.06267197,  0.17785825],\n",
              "        [ 0.0235547 , -0.02224658, -0.19540118, ..., -0.16289224,\n",
              "          0.10448419,  0.11331768],\n",
              "        ...,\n",
              "        [-0.10184544, -0.06701   ,  0.12802236, ...,  0.09268863,\n",
              "         -0.16879267, -0.09703728],\n",
              "        [-0.19247532, -0.13628536,  0.10741906, ...,  0.18835728,\n",
              "          0.03636011, -0.0418143 ],\n",
              "        [ 0.07045038, -0.07457916, -0.20245764, ..., -0.14473625,\n",
              "          0.0724908 , -0.08168623]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM9MoUfaHF8V"
      },
      "source": [
        "# Create custom layers\n",
        "The overall optimization process is similar to keras layers, but provides user additional control over module.\n",
        "You can create custom layers with or without trainable objects, gradient tape will skip or neglect all non-trainable parameters while calculating derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uP0oaffB3MX",
        "outputId": "8aca2fb3-8264-43be-ef53-80dea0535844"
      },
      "source": [
        "class IST597DenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_outputs):\n",
        "        super(IST597DenseLayer, self).__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.kernel = self.add_variable(\"kernel\",\n",
        "                                        shape=[int(input_shape[-1]),\n",
        "                                               self.num_outputs])\n",
        "\n",
        "    def call(self, input):\n",
        "        return tf.matmul(input, self.kernel)\n",
        "\n",
        "layer = IST597DenseLayer(128)\n",
        "print(layer(tf.zeros([128, 10])))\n",
        "print(layer.trainable_variables)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(128, 128), dtype=float32)\n",
            "[<tf.Variable 'ist597_dense_layer_4/kernel:0' shape=(10, 128) dtype=float32, numpy=\n",
            "array([[ 0.15198474,  0.06689079, -0.00597599, ..., -0.13580577,\n",
            "        -0.14869042,  0.04165433],\n",
            "       [ 0.04421245,  0.07853071, -0.20387553, ...,  0.12703057,\n",
            "         0.17095082,  0.04433741],\n",
            "       [-0.05372605,  0.11879389, -0.16131532, ...,  0.15864514,\n",
            "        -0.09661411,  0.1667379 ],\n",
            "       ...,\n",
            "       [ 0.14704143, -0.12887022, -0.20500785, ..., -0.04495606,\n",
            "         0.02551149, -0.13958366],\n",
            "       [-0.20395795, -0.18829784, -0.14605734, ...,  0.10494737,\n",
            "         0.11574872, -0.03081967],\n",
            "       [ 0.07394899,  0.06932463,  0.15476967, ..., -0.16231357,\n",
            "         0.15635408,  0.1387965 ]], dtype=float32)>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siAhgv6vH3va"
      },
      "source": [
        "# Defining your Resnet Blocks\n",
        "Here we will see how one can implement Post-activation and pre-activation resnet blocks.\n",
        "BN + Conv + relu = one set of trainable parameters (weight and biases for Conv and mean and variance for BN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veQg6XiiDDak",
        "outputId": "512f1782-f3db-4c0f-e2e0-18173e3b6bf5"
      },
      "source": [
        "class ResnetBlock(tf.keras.Model):\n",
        "    def __init__(self, kernel_size, filters):\n",
        "        super(ResnetBlock, self).__init__(name='')\n",
        "        filters1, filters2, filters3 = filters\n",
        "\n",
        "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
        "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
        "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
        "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        x = self.conv2a(input_tensor)\n",
        "        x = self.bn2a(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2b(x)\n",
        "        x = self.bn2b(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2c(x)\n",
        "        x = self.bn2c(x, training=training)\n",
        "\n",
        "        x += input_tensor\n",
        "        return tf.nn.relu(x)\n",
        "\n",
        "block = ResnetBlock(4, [64, 128, 256])\n",
        "print(block(tf.zeros([4, 64, 128, 256])))\n",
        "print([x.name for x in block.trainable_variables])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]], shape=(4, 64, 128, 256), dtype=float32)\n",
            "['conv2d/kernel:0', 'conv2d/bias:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'batch_normalization_2/gamma:0', 'batch_normalization_2/beta:0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoaEImoVDlSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406ee13e-fbe7-4c44-b886-f3c6d903a971"
      },
      "source": [
        "class ResnetBlockPre(tf.keras.Model):\n",
        "    def __init__(self, kernel_size, filters):\n",
        "        super(ResnetBlockPre, self).__init__(name='')\n",
        "        filters1, filters2, filters3 = filters\n",
        "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
        "        \n",
        "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
        "        \n",
        "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
        "        \n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        \n",
        "        x = self.bn2a(input_tensor, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2a(x)\n",
        "\n",
        "        \n",
        "        x = self.bn2b(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2b(x)\n",
        "\n",
        "       \n",
        "        x = self.bn2c(x, training=training)\n",
        "        x = self.conv2c(x)\n",
        "        \n",
        "        \n",
        "        x += input_tensor\n",
        "        return tf.nn.relu(x)\n",
        "\n",
        "block = ResnetBlockPre(4, [64, 128, 256])\n",
        "print(block(tf.zeros([4, 64, 128, 256])))\n",
        "print([x.name for x in block.trainable_variables])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]], shape=(4, 64, 128, 256), dtype=float32)\n",
            "['batch_normalization_3/gamma:0', 'batch_normalization_3/beta:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'batch_normalization_4/gamma:0', 'batch_normalization_4/beta:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'batch_normalization_5/gamma:0', 'batch_normalization_5/beta:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLoni5KmrFPD",
        "outputId": "ed7f5b01-86e5-45b4-f86b-b8373a6f6693"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
        "\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               latent_dim=32,\n",
        "               intermediate_dim=64,\n",
        "               name='encoder',\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z\n",
        "\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               original_dim,\n",
        "               intermediate_dim=64,\n",
        "               name='decoder',\n",
        "               **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    return self.dense_output(x)\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(tf.keras.Model):\n",
        "  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               original_dim,\n",
        "               intermediate_dim=64,\n",
        "               latent_dim=32,\n",
        "               name='autoencoder',\n",
        "               **kwargs):\n",
        "    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
        "    self.original_dim = original_dim\n",
        "    self.encoder = Encoder(latent_dim=latent_dim,\n",
        "                           intermediate_dim=intermediate_dim)\n",
        "    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var, z = self.encoder(inputs)\n",
        "    reconstructed = self.decoder(z)\n",
        "    # Add KL divergence regularization loss.\n",
        "    kl_loss = - 0.5 * tf.reduce_mean(\n",
        "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    self.add_loss(kl_loss)\n",
        "    return reconstructed\n",
        "\n",
        "\n",
        "original_dim = 784\n",
        "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(10):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed = vae(x_batch_train)\n",
        "      # Compute reconstruction loss\n",
        "      loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "      loss += sum(vae.losses)  # Add KLD regularization loss\n",
        "\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      print('step %s: mean loss = %s' % (step, loss_metric.result()))\n",
        "\n",
        "vae.save('vae')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Start of epoch 0\n",
            "step 0: mean loss = tf.Tensor(0.33800745, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.1249058, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.09881862, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.08892352, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.084020466, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.08074162, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.078637496, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.07705015, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.075907744, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.07489431, shape=(), dtype=float32)\n",
            "Start of epoch 1\n",
            "step 0: mean loss = tf.Tensor(0.074596465, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.07395722, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.0734612, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.072979204, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.07265025, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.072255164, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.07195949, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.071668126, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.07144146, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.07117561, shape=(), dtype=float32)\n",
            "Start of epoch 2\n",
            "step 0: mean loss = tf.Tensor(0.07110219, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.07092784, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.07079455, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.07065036, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.070554875, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.07039459, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.070289604, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.07016519, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.070070095, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06994081, shape=(), dtype=float32)\n",
            "Start of epoch 3\n",
            "step 0: mean loss = tf.Tensor(0.06990913, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06983208, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.06977508, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06970113, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.0696651, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.06957518, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.06951877, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.06945309, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06939645, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06931805, shape=(), dtype=float32)\n",
            "Start of epoch 4\n",
            "step 0: mean loss = tf.Tensor(0.06929992, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06925302, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.069227986, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06918653, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.069163956, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.069109775, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.069075026, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.0690309, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06899291, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06894061, shape=(), dtype=float32)\n",
            "Start of epoch 5\n",
            "step 0: mean loss = tf.Tensor(0.06892909, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06890244, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.06888317, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.068853825, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.068844676, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.0688041, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.06877688, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.06874915, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06872412, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06868432, shape=(), dtype=float32)\n",
            "Start of epoch 6\n",
            "step 0: mean loss = tf.Tensor(0.06867714, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06865719, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.068647824, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06862663, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.06862213, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.06859074, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.06857436, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.0685507, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.068533994, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06850126, shape=(), dtype=float32)\n",
            "Start of epoch 7\n",
            "step 0: mean loss = tf.Tensor(0.06849458, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06848026, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.06847412, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06845882, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.06845661, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.06843303, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.06841744, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.06840013, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06838484, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.06835976, shape=(), dtype=float32)\n",
            "Start of epoch 8\n",
            "step 0: mean loss = tf.Tensor(0.0683557, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06834452, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.068338625, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06832764, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.06832713, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.06830778, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.06829672, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.06828059, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06827224, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.0682473, shape=(), dtype=float32)\n",
            "Start of epoch 9\n",
            "step 0: mean loss = tf.Tensor(0.06824418, shape=(), dtype=float32)\n",
            "step 100: mean loss = tf.Tensor(0.06823604, shape=(), dtype=float32)\n",
            "step 200: mean loss = tf.Tensor(0.06823341, shape=(), dtype=float32)\n",
            "step 300: mean loss = tf.Tensor(0.06822204, shape=(), dtype=float32)\n",
            "step 400: mean loss = tf.Tensor(0.0682249, shape=(), dtype=float32)\n",
            "step 500: mean loss = tf.Tensor(0.06820779, shape=(), dtype=float32)\n",
            "step 600: mean loss = tf.Tensor(0.068198904, shape=(), dtype=float32)\n",
            "step 700: mean loss = tf.Tensor(0.068185605, shape=(), dtype=float32)\n",
            "step 800: mean loss = tf.Tensor(0.06817638, shape=(), dtype=float32)\n",
            "step 900: mean loss = tf.Tensor(0.0681593, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: vae/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: vae/assets\n"
          ]
        }
      ]
    }
  ]
}